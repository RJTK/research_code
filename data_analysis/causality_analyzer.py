import pickle
import sys

import pandas as pd
import numpy as np
import spams as spm

from scipy import optimize as sp_opt
from random import shuffle
from matplotlib import pyplot as plt
from matplotlib import colors as mpl_colors
from matplotlib.widgets import Slider
from spams import lasso
from progressbar import Bar, Percentage, ETA, ProgressBar, SimpleProgress
from resource_limiter import limit_memory_as

#This fits together various routines used for analyzing GC graphs.
#The data is expected to be already centered and stationary
#This is easy to garauntee since I'm using synthetic data...

DATA_DIR = '/home/ryan/Documents/academics/research/' \
           'granger_causality/software/datasets/synthetic/'

F_TRAIN = 0.7 #% for training
F_TEST = 0.9 #F_TEST - F_TRAIN = % for testing
F_VERIF = 1.0 #This should always be 1

def load_data(file_name):
  '''
  Loads a previously pickled set of data.  The loaded object should
  be a dictionary containing the parameters, the underlying causality
  graph, and the actual data.  This is designed to load in data
  generated by the data_synthesis routines.  See that file for details.
  '''
  f = open(file_name, 'rb')
  A = pickle.load(f)
  return A

def build_YZ(D, I, p):
  '''
  Builds the Y (output) and Z (input) matrices for the model from
  a pandas dataframe D and set of indices I.  We need to also provide
  the lag length of the model, p.

  Y_hat = BZ

  D: Pandas dataframe of data
  I: Set of indices to use
  p: Model lag length
  '''
  T = len(I)
  if T == 0:
    return np.array([]), np.array([])
  Y = np.array(D[p:]).T

  Z = np.array(D.ix[I[p - 1: : -1]]).flatten()
  for k in range(1, T - p):
    Zk = np.array(D.ix[I[k + p - 1: k - 1: -1]]).flatten()
    Z = np.vstack((Z, Zk))
  Z = Z.T
  return Y, Z

def OLS(Y, Z):
  '''
  B = argmin_B ||Y - BZ||_F^2
  '''
  ZZT = np.dot(Z, Z.T)
  ZZT_inv = np.linalg.inv(ZZT)
  YZT = np.dot(Y, Z.T)
  B = np.dot(YZT, ZZT_inv)
  return B

def OLS_tikhonov(Y, Z, lmbda, ZZT = None):
  '''
  B = argmin_B ||Y - BZ||_F^2 + lmbda||B||_F^2
  '''
  if ZZT is None:
    ZZT = np.dot(Z, Z.T)
  tmp = lmbda*np.eye(ZZT.shape[0]) + ZZT
  tmp = np.linalg.inv(tmp)
  YZT = np.dot(Y, Z.T)
  B = np.dot(YZT, tmp)
  return B

def spams_lasso(Y, Z, lmbda):
  '''
  B = argmin_B ||Y - BZ||_F^2 + lmbda||B||_1
  '''
  Y_spm = np.asfortranarray(Y.T)
  Z_spm = np.asfortranarray(Z.T)
  B = lasso(Y_spm, Z_spm, lambda1 = lmbda, lambda2 = 0,
            mode = spm.PENALTY)
  B = B.toarray()
  return B.T

def cx_validate_opt(Y_train, Z_train, Y_test, Z_test, f, lmbda_min = 0,
                    lmbda_max = 100, **kwargs):
  '''
  Train the model given by f on the data (Y_train, Z_train) and uses
  scipy.optimize to search for the set of hyper parameters that
  provide the best performance on the test data (Y_test, Z_test).

  Currently set up to just minimize over a single hyperparameter.
  '''
  print 'Crossvalidating %s' % f.__name__
  def opt_f(lmbda):
    B = f(Y_train, Z_train, lmbda, **kwargs)
    Y_hat_test = np.dot(B, Z_test)
    err_test = np.linalg.norm(Y_test - Y_hat_test, 'fro')**2
    print 'lmbda = %f\r' % lmbda,
    sys.stdout.flush()
    return err_test
  
  #Replace this with optimize.minimize for a vector of params
  result = sp_opt.minimize_scalar(opt_f,
                                  bounds = (lmbda_min, lmbda_max),
                                  method = 'brent')
  lmbda_star = result.x
  print 'lmbda_star = %f\n' % lmbda_star

  B_star = f(Y_train, Z_train, lmbda_star, **kwargs)
  Y_hat_star = np.dot(B_star, Z_test)
  err_star = np.linalg.norm(Y_test - Y_hat_star, 'fro')**2

  return B_star, lmbda_star, err_star
  
#THIS IS DEPRECATED, cx_validate_opt is way better
def cx_validate(Y_train, Z_train, Y_test, Z_test, Lmbda, f, **kwargs):
  '''
  Train the model given by f on the data (Y_train, Z_train) and then
  cycles through the set of parameters given in Lmbda to find the best
  performing set of hyperparameters on the test data (Y_test, Z_test).
  '''
  errs = []
  widgets = [Percentage(), ' ', Bar(), ' ', ETA()]
  pbar = ProgressBar(widgets = widgets, maxval = len(Lmbda))
  pbar.start()
  min_err = np.inf
  for lmbda_i, lmbda in enumerate(Lmbda):
    pbar.update(lmbda_i)
    B = f(Y_train, Z_train, lmbda, **kwargs)
    Y_hat_test = np.dot(B, Z_test)
    err_test = np.linalg.norm(Y_test - Y_hat_test, 'fro')**2
    errs.append(err_test)
    try:
      if(err_test < min_err):
        B_star = B
        min_err = err_test
        lmbda_star = lmbda
    except NameError:
      B_star = B
      min_err = err_test
      lmbda_star = lmbda
  pbar.finish()
  return B_star, lmbda_star, errs

def split_data(D, T, F_train, F_test, F_verif):
  '''Splits the data into separate train, test and verify pieces'''
  I_train = D.index[0:int(T*F_train)]
  I_test = D.index[int(T*F_train):int(T*F_test)]
  I_verif = D.index[int(T*F_test):int(T*F_verif)]

  T_train = len(I_train)
  T_test = len(I_test)
  T_verif = len(I_verif)

  D_train = D.ix[I_train].copy()
  D_test = D.ix[I_test].copy()
  D_verif = D.ix[I_verif].copy()

  return D_train, I_train, D_test, I_test, D_verif, I_verif

def fit_var(A):
  G, D, p, n, T = A['G'], A['D'], A['p'], A['n'], A['T']

  #Baseline error measures
  def true_mean_err(Y):
    #Since the true mean is zero
    return np.linalg.norm(Y, 'fro')**2

  def training_mean_err(Y_train, Y_test):
    Y_mean = Y_train.mean(axis = 1)
    Y_mean = np.repeat(Y_mean, Y_test.shape[1]).reshape(Y_mean.shape[0],
                                                        Y_test.shape[1])
    err_mean_test = np.linalg.norm(Y_test - Y_mean, 'fro')**2
    return err_mean_test

  def prev_point_err(D, I):
    Y_prev, Y_hat_prev = build_YZ(D, I, 1)
    err_prev = np.linalg.norm(Y_prev - Y_hat_prev, 'fro')**2
    return err_prev

  def model_err(B, Y, Z):
    Y_hat = np.dot(B, Z)
    err = np.linalg.norm(Y_hat - Y, 'fro')**2
    return err
    

  #-----------Split up the data-------------
  D_train, I_train, D_test, I_test, D_verif, I_verif = split_data(
  D, T, F_TRAIN, F_TEST, F_VERIF)

  Y_all, Z_all = build_YZ(D, D.index, p)
  Y_train, Z_train = build_YZ(D_train, I_train, p)
  Y_test, Z_test = build_YZ(D_test, I_test, p)
  Y_verif, Z_verif = build_YZ(D_verif, I_verif, p)

  #Base line tests
  err_0_test = true_mean_err(Y_test)
  err_0_verif = true_mean_err(Y_verif)
  err_mean_test = training_mean_err(Y_train, Y_test)
  err_mean_verif = training_mean_err(Y_train, Y_verif)
  err_prev_test = prev_point_err(D_test, I_test)
  err_prev_verif = prev_point_err(D_verif, I_verif)

  B_OLS = OLS(Y_train, Z_train)
  B_OLST, lmbda_OLST_star, err_star = cx_validate_opt(Y_train, Z_train,
                                                      Y_test, Z_test,
                                                      OLS_tikhonov,
                                                      lmbda_min = 0.0001,
                                                      lmbda_max = 5000)
  B_LASSO, lmbda_LASSO_star, err_star = cx_validate_opt(Y_train, Z_train,
                                                        Y_test, Z_test,
                                                        spams_lasso,
                                                        lmbda_min = 0.0001,
                                                        lmbda_max = 5000)
  err_OLS_test = model_err(B_OLS, Y_test, Z_test)
  err_OLS_verif = model_err(B_OLS, Y_verif, Z_verif)
  err_OLST_test = model_err(B_OLST, Y_test, Z_test)
  err_OLST_verif = model_err(B_OLST, Y_verif, Z_verif)
  err_LASSO_test = model_err(B_LASSO, Y_test, Z_test)
  err_LASSO_verif = model_err(B_LASSO, Y_verif, Z_verif)  

  N = Y_test.size
  print 'err_0_test: %f' % (err_0_test / N)
  print 'err_mean_test: %f' % (err_mean_test / N)
  print 'err_prev_test: %f' % (err_prev_test / N)
  print 'err_OLS_test: %f' % (err_OLS_test / N)
  print 'err_OLST_test: %f' % (err_OLST_test / N)
  print 'err_LASSO_test: %f' % (err_LASSO_test / N)

  print '\n',

  N = Y_verif.size
  print 'err_0_verif: %f' % (err_0_verif / N)
  print 'err_mean_verif: %f' % (err_mean_verif / N)
  print 'err_prev_verif: %f' % (err_prev_verif / N)
  print 'err_OLS_verif: %f' % (err_OLS_verif / N)
  print 'err_OLST_verif: %f' % (err_OLST_verif / N)
  print 'err_LASSO_verif: %f' % (err_LASSO_verif / N)

  return B_OLS, B_OLST, B_LASSO

def ex1():
  np.random.seed(1)
  for pi in range(1, 4):
    for Ti in [200, 500, 1000]:
      A = load_data(DATA_DIR + 'iidG_ER_p%d_T%d.pkl' % (pi, Ti))
      print '----------DATA SET (p = %d, T = %d)----------' % (pi, Ti)
      B_OLS, B_OLST, B_LASSO = fit_var(A)
      print '\n',
  return

def causality_graph_comparison(A):
  np.random.seed(1)
  G, D, p, n, T = A['G'], A['D'], A['p'], A['n'], A['T']

  D_train, I_train, D_test, I_test, D_verif, I_verif = split_data(
  D, T, F_TRAIN, F_TEST, F_VERIF)

  Y_all, Z_all = build_YZ(D, D.index, p)
  Y_train, Z_train = build_YZ(D_train, I_train, p)
  Y_test, Z_test = build_YZ(D_test, I_test, p)
  Y_verif, Z_verif = build_YZ(D_verif, I_verif, p)

  #--------FIT MODELS-------------
  B_OLST, lmbda_OLST_star, lmbda_star = cx_validate_opt(Y_train, Z_train,
                                                        Y_test, Z_test,
                                                        OLS_tikhonov,
                                                        lmbda_min = 0.0001,
                                                        lmbda_max = 5000)
  B_LASSO, lmbda_LASSO_star, lmbda_star = cx_validate_opt(Y_train, Z_train,
                                                          Y_test, Z_test,
                                                          spams_lasso,
                                                          lmbda_min = 0.0001,
                                                          lmbda_max = 5000)

  #---------DISPLAY GRAPHS------------
  num_nodes = B_OLST.shape[0]
  A_OLST = sum(np.abs(B_OLST[0 : num_nodes,
                             k*num_nodes : (k + 1)*num_nodes])
               for k in range(0, p))
  A_OLST = (np.abs(A_OLST) > 0)

  print A_OLST

  #LASSO adj matrix
  A_LASSO = sum(np.abs(B_LASSO[0 : num_nodes,
                               k*num_nodes : (k + 1)*num_nodes])
                for k in range(0, p))
  A_LASSO = (np.abs(A_LASSO) > 0)

  fig, ax = plt.subplots(1, 3)
  ax[0].set_title('OLST, $\epsilon$ threshold')
  ax[1].set_title('LASSO')
  ax[2].set_title('Diff (Black indicates no difference)')
  plt.subplots_adjust(bottom = 0.3)
  A_OLST_img = ax[0].imshow(A_OLST, cmap = 'Greys')
  A_LASSO_img = ax[1].imshow(A_LASSO, cmap = 'Greys')
  diff_img = ax[2].imshow((A_OLST) == A_LASSO, cmap = 'Greys')

  eps_axis = plt.axes([.1, .05, .85, .05])
  lmbda_t = plt.axes([.1, .15, .85, .05])
  lmbda_l = plt.axes([.1, .25, .85, .05])
  eps_slider = Slider(eps_axis, '$\epsilon$', 0, np.max(B_OLST), valinit = 0)
  lmbda_t_slider = Slider(lmbda_t, '$\lambda_{l2}$', 0, 50*lmbda_OLST_star,
                          valinit = lmbda_OLST_star)

  lmbda_l_slider = Slider(lmbda_l, '$\lambda_{l1}$', 0, 50*lmbda_LASSO_star,
                          valinit = lmbda_LASSO_star)

  def update(x):
    eps = eps_slider.val
    lmbda_t = lmbda_t_slider.val
    lmbda_l = lmbda_l_slider.val

    B_OLST = OLS_tikhonov(Y_train, Z_train, lmbda_t)
    A_OLST = sum(np.abs(B_OLST[0 : num_nodes,
                               k*num_nodes : (k + 1)*num_nodes])
                 for k in range(0, p))
    A_OLST = np.array(np.abs(A_OLST) > eps, dtype = np.int)
    A_OLST_img.set_data(A_OLST)

    B_LASSO = spams_lasso(Y_train, Z_train, lmbda_l)
    A_LASSO = sum(np.abs(B_LASSO[0 : num_nodes,
                                 k*num_nodes : (k + 1)*num_nodes])
                  for k in range(0, p))
    A_LASSO = np.array(np.abs(A_LASSO) > 0, dtype = np.int)
    A_LASSO_img.set_data(A_LASSO)

    diff = np.array(A_OLST == A_LASSO, dtype = np.int)

    diff_img.set_data(diff)

    A_OLST_img.set_cmap('Greys')
    A_LASSO_img.set_cmap('Greys')
    diff_img.set_cmap('Greys')
    A_OLST_img.autoscale()
    A_LASSO_img.autoscale()
    diff_img.autoscale()
    fig.canvas.draw()
    return

  eps_slider.on_changed(update)
  lmbda_t_slider.on_changed(update)
  lmbda_l_slider.on_changed(update)

  plt.show()
  return

def discrete_cmap(N, base_cmap=None):
  """Create an N-bin discrete colormap from the specified input map"""

  # Note that if base_cmap is a string or None, you can simply do
  #    return plt.cm.get_cmap(base_cmap, N)
  # The following works for string, None, or a colormap instance:

  base = plt.cm.get_cmap(base_cmap)
  color_list = base(np.linspace(0, 1, N))
  cmap_name = base.name + str(N)
  return base.from_list(cmap_name, color_list, N)

def causality_graph_LASSO(A):
  np.random.seed(1)
  G, D, p, n, T = A['G'], A['D'], A['p'], A['n'], A['T']
  B = A['B']

  plt.imshow(np.hstack(B))
  plt.colorbar()
  plt.show()


  D_train, I_train, D_test, I_test, D_verif, I_verif = split_data(
  D, T, F_TRAIN, F_TEST, F_VERIF)

  Y_train, Z_train = build_YZ(D_train, I_train, p)
  Y_test, Z_test = build_YZ(D_test, I_test, p)

  #--------FIT MODEL-------------
  B_LASSO, lmbda_LASSO_star, lmbda_star = cx_validate_opt(Y_train, Z_train,
                                                          Y_test, Z_test,
                                                          spams_lasso,
                                                          lmbda_min = 0.0001,
                                                          lmbda_max = 5000)
  #LASSO adj matrix
  cm =  mpl_colors.ListedColormap(['blue', 'white', 'black', 'red'])
  bounds = [-1.1, 0, 1, 2, 2.1]
  norm = mpl_colors.BoundaryNorm(bounds, cm.N)
  A_LASSO = sum(np.abs(B_LASSO[0 : n, k*n : (k + 1)*n])
                for k in range(0, p))
  A_LASSO = (np.abs(A_LASSO) > 0)

  fig, ax = plt.subplots(1, 1)
  ax.set_title('LASSO')
  plt.subplots_adjust(bottom = 0.2)
  A_LASSO_img = ax.imshow(2*A_LASSO - G, cmap = cm, norm = norm,
                          interpolation = 'nearest')
  fig.colorbar(A_LASSO_img, cmap = cm, norm = norm, boundaries = bounds,
               ticks = [-1, 0, 1, 2])
  lmbda = plt.axes([.1, .05, .85, .05])
  lmbda_slider = Slider(lmbda, '$\lambda$', 0, 50*lmbda_LASSO_star,
                        valinit = lmbda_LASSO_star)
  def update(x):
    lmbda = lmbda_slider.val
    B_LASSO = spams_lasso(Y_train, Z_train, lmbda)
    A_LASSO = sum(np.abs(B_LASSO[0 : n, k*n : (k + 1)*n])
                  for k in range(0, p))
    A_LASSO = np.array(np.abs(A_LASSO) > 0, dtype = np.int)
    A_LASSO_img.set_data(2*A_LASSO - G)
    A_LASSO_img.autoscale()
    fig.canvas.draw()
    return
  
  lmbda_slider.on_changed(update)
  plt.show()
  

if __name__ == '__main__':
  limit_memory_as(int(7000e6))
  np.random.seed(1)
  A = load_data(DATA_DIR + 'iidG_ER_p2_T500.pkl')
#  A = load_data(DATA_DIR + 'iidG_ER_p2_T500_n5.pkl')
#  A = load_data(DATA_DIR + 'iidG_ER_p2_T20000_n100.pkl')
#  causality_graph_comparison(A)
  causality_graph_LASSO(A)
